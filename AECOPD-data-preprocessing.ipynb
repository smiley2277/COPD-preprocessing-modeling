{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm, trange\n",
    "import itertools\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm \n",
    "from sklearn import preprocessing\n",
    "\n",
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        print(path+'目錄已存在')\n",
    "        \n",
    "def read(file):\n",
    "    csv = r\".csv\"\n",
    "    xlsx = r\".xlsx\"\n",
    "    if file.endswith(csv):\n",
    "        with open(file, encoding='unicode_escape') as q:\n",
    "            output_df = pd.read_csv(q)\n",
    "    elif file.endswith(xlsx):\n",
    "        with open(file, 'rb') as q:\n",
    "            output_df = pd.read_excel(q)\n",
    "    return output_df\n",
    "    \n",
    "def feature_distribution_plot(non_padding_df):   \n",
    "    \"\"\"\n",
    "    non_padding_df : DataFame type, should be in non-padding type\n",
    "    output : .png type, in the folder 'feature_distribution'\n",
    "    \"\"\"\n",
    "    filepath = \"./feature_distribution_0127\"\n",
    "    mkdir(filepath)\n",
    "    while os.path.exists(filepath):\n",
    "        if ('serial' in non_padding_df.columns):\n",
    "            non_padding_df = non_padding_df.drop(columns = ['serial'])\n",
    "        if ('id' in non_padding_df.columns):\n",
    "            non_padding_df = non_padding_df.drop(columns = ['id'])\n",
    "        if ('date' in non_padding_df.columns):\n",
    "            non_padding_df = non_padding_df.drop(columns = ['date'])\n",
    "        non_padding_df = non_padding_df.dropna()\n",
    "        \n",
    "        for index in non_padding_df.columns:\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sns.boxplot(x='value', y=index, data= non_padding_df)\n",
    "            my_file = '0127_'+index+'.png'\n",
    "            plt.savefig(my_file)\n",
    "            \n",
    "def append_id(life_df):\n",
    "    \"\"\"\n",
    "    function : combine the 'PR_xxx' into lifestyle data\n",
    "    life_df : DataFame type, should contain column named serial.\n",
    "    output : DataFame type, contain column named id.\n",
    "    \"\"\"\n",
    "    serial_id = []\n",
    "    serial_life = life_df['serial'].tolist() #serial\n",
    "    for i in range(0, len(serial_life)):\n",
    "        con = serial_life[i]\n",
    "        pr = str(map_df[map_df['id'] == con]['serial'].values).strip('[]').strip(\"''\")\n",
    "        serial_id.append(pr)\n",
    "    id_df = pd.DataFrame(serial_id,columns=['id'])\n",
    "    comp = pd.concat([life_df,id_df],axis=1)\n",
    "    print('ID combination finish')\n",
    "    comp_D = []\n",
    "    comp_Date = comp['date'].tolist()\n",
    "    for i in range(0, len(comp_Date)):\n",
    "        con = comp_Date[i]\n",
    "        pr = str(con).strip('00:00:00+08:00').strip('T')\n",
    "        comp_D.append(pr)\n",
    "    comp['date'] = pd.DataFrame(comp_D)\n",
    "    return comp \n",
    "\n",
    "def append_serial(life_df):\n",
    "    \"\"\"\n",
    "    function : combine the 'serial' into other data\n",
    "    life_df : DataFame type, should contain column named serial.\n",
    "    output : DataFame type, contain column named id.\n",
    "    \"\"\"\n",
    "    serial_id = []\n",
    "    serial_life = life_df['id'].tolist() #serial\n",
    "    for i in range(0, len(serial_life)):\n",
    "        con = serial_life[i]\n",
    "        pr = str(map_df[map_df['serial'] == con]['id'].values).strip('[]').strip(\"''\")\n",
    "        serial_id.append(pr)\n",
    "        print(con, pr)\n",
    "    id_df = pd.DataFrame(serial_id,columns=['serial'])\n",
    "    comp = pd.concat([life_df,id_df],axis=1)\n",
    "    print('ID combination finish')\n",
    "    return comp \n",
    "\n",
    "def map_ques(life_df, ques_df):\n",
    "    \"\"\"\n",
    "    function : mapping questionnaire into lifestyle dataframe.\n",
    "    life_df : DataFame type, should contain column named serial.\n",
    "    output : DataFame type, contain column named id.\n",
    "    \"\"\"\n",
    "    # deal with questionnaire\n",
    "    sip = ques_df.copy()\n",
    "    sip = sip.dropna(subset=['date'])\n",
    "    ques_column = ['Age','id','Mmrc','1_move', '2_selfcare', '3_activity', '4_uncomfor', '5_depress','今天健康狀況','date',\n",
    "        'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7', 'CAT_8', 'CAT_total','AE 日期 (二週內)','anti-biotics',\n",
    "       'steroid', 'bronchodilators']\n",
    "    sip = sip.reindex(columns=ques_column)\n",
    "#     #date of questionnaire format modifing\n",
    "#     ques_D = []\n",
    "#     ques_Date = ques_df['date'].tolist()\n",
    "#     for i in range(0, len(ques_Date)):\n",
    "#         con = ques_Date[i]\n",
    "#         pr = str(con).strip(' 00:00:00')\n",
    "#         ques_D.append(pr)\n",
    "#     ques_df['date'] = pd.DataFrame(ques_D)\n",
    "\n",
    "    sip['date'] = pd.to_datetime(sip['date'], errors='coerce')\n",
    "    life_df['date'] = pd.to_datetime(life_df['date'], errors='coerce')\n",
    "#     rrr = sip[sip['date'].isna()].index\n",
    "#     print(rrr)\n",
    "    comp = pd.merge(life_df, sip, on=['id','date'], how='left')\n",
    "    res = comp.to_excel('./process_file/map_ques.xlsx',index=False)\n",
    "    return comp\n",
    "\n",
    "def check_missing_perc(df):\n",
    "    '''\n",
    "    function : to check empty percentage of the dataframe\n",
    "    df : DataFrame type\n",
    "    '''\n",
    "    qwe=df.isnull().sum().sort_values(ascending=False)/len(df)\n",
    "    asd=qwe[qwe>0]\n",
    "    print(asd)\n",
    "    return asd\n",
    "\n",
    "def extract_hos_time(seg_df, lif_df):\n",
    "    '''\n",
    "    function : extract collecting patient time\n",
    "    seg_df : DataFrame type, contain columns named 'id','serial','起','迄'\n",
    "    lif_df : DataFrame type\n",
    "    '''\n",
    "    seg_df = seg_df.reindex(columns = ['id','serial','起','迄'])\n",
    "    lif_df['date'] = pd.to_datetime(lif_df['date'])\n",
    "    lif_df.index = lif_df['date']\n",
    "    lif_range = []\n",
    "    for ser in lif_df['serial'].unique():\n",
    "        con = lif_df[lif_df['serial'] == ser]\n",
    "        con = con.sort_index()\n",
    "        if len(seg_df[seg_df['serial'] == ser]['起'].values) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            start_date = seg_df[seg_df['serial'] == ser]['起'].values[0]\n",
    "            end_date = seg_df[seg_df['serial'] == ser]['迄'].values[0]\n",
    "            date_range = con[start_date : end_date]\n",
    "            lif_range.append(date_range)\n",
    "    vom = pd.DataFrame()\n",
    "    for ind in lif_range:\n",
    "        vom = pd.concat([vom, ind], axis = 0)\n",
    "    return vom\n",
    "\n",
    "def merge_open_data(ope_df, lif_df):\n",
    "    '''\n",
    "    function : merge open data\n",
    "    ope_df : DataFrame type, extracted open data\n",
    "    lif_df : DataFrame type\n",
    "    '''\n",
    "    ope_df['date'] = pd.to_datetime(ope_df['date'])\n",
    "    ope_df.index.name = None\n",
    "    lif_df.index.name = None\n",
    "    comp = pd.merge(lif_df, ope_df, on=['id','date','serial'], how='left')\n",
    "    rest = comp.to_excel('./process_file/merge_open_data.xlsx', index=False)\n",
    "    return comp\n",
    "\n",
    "def listing_day(ae_day):\n",
    "    time_sec = []\n",
    "    for i in range(7):\n",
    "        n = timedelta(days=i)\n",
    "        x_day = ae_day - n\n",
    "        time_sec.append(x_day)\n",
    "    return time_sec\n",
    "\n",
    "def labeling(ques_df, life_df):\n",
    "    '''\n",
    "    function : labeling ground truth\n",
    "    ques_df : DataFrame type, contain column named 'AE 日期 (二週內)'\n",
    "    life_df : DataFrame type\n",
    "    '''\n",
    "    columns = ['id','AE 日期 (二週內)']\n",
    "    ae_date = ques_df[ques_df['AE 日期 (二週內)'].notnull()][columns]\n",
    "    ae = []\n",
    "    for id,content in ae_date.values :\n",
    "        time_sec = listing_day(content)\n",
    "        ae_data = ques_df[ques_df['AE 日期 (二週內)'] == content]\n",
    "        uni_data = ae_data[ae_data['id'] == id]\n",
    "        ae_whom = str(uni_data['id'].values).strip('[]').strip(\"''\")\n",
    "        for i in time_sec:  \n",
    "            ae_when = str(i).strip('00:00:00').strip(' ')\n",
    "            ae_value = True\n",
    "            ae.append([ae_whom,ae_when,ae_value])\n",
    "    ae = pd.DataFrame(ae, columns = ['id', 'date', 'value'])\n",
    "    ae['date'] = pd.to_datetime(ae['date'])\n",
    "    ae = ae.drop_duplicates()\n",
    "    comp = pd.merge(life_df, ae, on=['id','date'], how = 'left')\n",
    "    # input false\n",
    "    pend_num = comp['value'].shape[0]\n",
    "    value_queue = []\n",
    "    for i in comp['value']:\n",
    "        if i == True:\n",
    "            value_queue.append(True)\n",
    "        else:\n",
    "            value_queue.append(False)\n",
    "    comp['value'] = pd.DataFrame(value_queue)\n",
    "#     res = comp.to_excel('./process_file/labeling.xlsx', index=False)\n",
    "    return comp\n",
    "\n",
    "def patient_choosing(patient_list):\n",
    "    if patient_list == 'patient':\n",
    "        patient_list = {'1':'7KL867','2':'k8toglom','3':'7MBVPK','4':'7WBRYJ','6':'kczxv1yv','7':'kavrxlv0',\n",
    "                  '8':'kalrxuor','9':'7TNLYF','10':'7RWBXQ','11':'kadbivrk','12':'7RW6GF','13':'kbbnvugt','14':'7W3P7C',\n",
    "                  '15':'kceij9pg','16':'k47r2n3q','17':'k2x2ub0g','18':'kdtwh1d2','19':'ke2r3z50','20':'k7odj3j9','21':'k47r9ign',\n",
    "                  '22':'k8tmb0x8','23':'k3tipk3h','24':'7MB7NF','25':'6XJ5LS','26':'k5g3nwmg','27':'7MJ773','28':'kc04axbp',\n",
    "                  '29':'k4nsgd6d','30':'kczuibsw','31':'7S2DJN','32':'kam6rhh8','33':'k9tmgc0q','34':'7L87WN','35':'karruqcp',\n",
    "                  '36':'k7ohrrzh','37':'k8toglom','38':'k8nsxrf8','5':'k372eowt','126':'7J4LSJ',\n",
    "                  '43':'k997cdna','44':'kam6ludx','45':'kcfkysl2','46':'7J6Y9D','47':'k9rss2np','48':'k3tj79oz','49':'kcpnwq06',\n",
    "                  '50':'kaz56899','51':'k9j8fn98','52':'k40k27he','53':'k9rsmyeu','54':'k99aupqm','55':'ka3pehhb','56':'7M6XMQ',\n",
    "                  '57':'7FZ7TY','58':'kaorfe00','59':'ka2aezge','60':'k4m6ts8z','61':'kbbm2myc','62':'7SMYHT','63':'7KMNHS',\n",
    "                  '64':'kc04fomc','65':'k8xykf3o','66':'kb0dp1ue','67':'k99b5sby','68':'7RSN5M','69':'7SMD93','70':'k8nshpi3',\n",
    "                  '71':'7MKC44','72':'7GYPY5','73':'kdr4e4wl','74':'7MKFFR','75':'7JZSZL','76':'ke0no4tg','77':'7GTFN9',\n",
    "                  '78':'k7o6s8pl','79':'7HV84W','81':'7HMG67','82':'kdej4s8c','83':'7MHM4W','84':'7LXXQD',\n",
    "                  '85':'7NFJKW','86':'783XWR','87':'7RMTVM','88':'k3dq74l6','89':'k93nsgzp','90':'2G7926','91':'kb1q0gqq',\n",
    "                  '92':'7NQLKZ','93':'7LCWVV','94':'6WG8H8','95':'karou8wm','96':'7LHHQF','97':'7T3LYW','98':'kaerkis0',\n",
    "                  '99':'karl5ctc','100':'k4m77n9j','101':'7L2KQW','102':'k3tiv86l','103':'k8nsub3m','104':'kc5xiieq','105':'7MB7XP',\n",
    "                  '106':'kdtmxjjl','107':'k6iw0ywz','108':'k2tp0e2v','109':'k7o7u15e','110':'7RSHHF','111':'k4m7mszd','112':'7WGXYW',\n",
    "                  '113':'kb1potm8','114':'kdi91iwz','115':'7JF7QJ','116':'7SDWSM','117':'7N93QH','119':'k2tpoc4a',\n",
    "                  '120':'k3fnnnjn','121':'7Q8ZYG','122':'7MPFHS','123':'k42gwti3','124':'7V9Q5S','125':'7L87TF','127':'kcye41i8','128':'7KDJNB','129':'kc5vhfe0','130':'kb0gvtyo','131':'kdjilbd3',\n",
    "                  '132':'k528rnky','133':'kdtw95p2','134':'kdeh9tt6','135':'kbbkpfoa','136':'kc5w4ob9','137':'7JWD4K','138':'ke2hfb32','139':'kceiba9o',\n",
    "                       '139':'kgd66hpt','140':'khzm9k0j','141':'klvk4w7v','142':'km4vxg3p','143':'kn0dlkrs',\n",
    "                       '144':'knfrg28p','145':'knfawjjo','146':'knfak07e','147':'kjtmvtd8','148':'kippk2c8','149':'kil0ffs0','150':'kia2b6g6',\n",
    "                       '151':'ki5k4qzl','152':'ki3xbmyf','153':'khzsf37x','154':'khx6uk8f','155':'khpxb8hx','156':'khpx3qrg','157':'khjvrn4u',\n",
    "                       '158':'kgq2lzjt','159':'kh604tvu','160':'kg8xc28s','161':'kg7pewto','162':'kfhtgs25','163':'kfdrhdqz','164':'kf54thjh','165':'kf3msaay',\n",
    "                       '166':'kev6cplc','167':'kev689ab','168':'kerwnszh','169':'ke9qjtwg'}\n",
    "    elif patient_list == 'NN_patient':\n",
    "        patient_list = {'1':'karl5ctc','2':'karou8wm','3':'karruqcp','4':'kb0dp1ue','5':'kb1potm8','6':'kb1q0gqq','7':'kbbkpfoa',\n",
    "                     '8':'kbbm2myc','9':'kbbnvugt','10':'kc5vhfe0','11':'kc5w4ob9','12':'kc5xiieq','13':'kcfkysl2','14':'kcpnwq06',\n",
    "                     '15':'kczuibsw','16':'kczxv1yv','17':'kdej4s8c','18':'kdeh9tt6','19':'kdtw95p2','20':'kdtwh1d2','21':'ke2r3z50',\n",
    "                       '22':'kgd66hpt','23':'khzm9k0j','24':'klvk4w7v','25':'km4vxg3p','26':'kn0dlkrs'}\n",
    "    elif patient_list == 'NTU_patient':\n",
    "        patient_list = {'1':'k773avug','2':'k7o7u15e','3':'k8nshpi3','4':'k7o6s8pl','5':'k7ohrrzh','6':'k8nsub3m','7':'k8nsxrf8',\n",
    "                      '8':'k8tmb0x8','9':'k8toglom','10':'k8xykf3o','11':'k997cdna','12':'k99aupqm','13':'k99b5sby','14':'ka3pehhb',\n",
    "                      '15':'k93nsgzp','16':'k9j8fn98','17':'k9rss2np','18':'k9rsmyeu','19':'kam6rhh8','20':'k9tmgc0q','21':'ka2aezge',\n",
    "                      '22':'kalrxuor','23':'kals4gfo','24':'kadbivrk','25':'kaerkis0','26':'kam6ludx','27':'kaorfe00','28':'kavrxlv0',\n",
    "                      '29':'kaz56899','30':'kb0gvtyo','31':'kc04axbp','32':'kc04fomc','34':'kceij9pg',\n",
    "                      '36':'kcye41i8','37':'kdi91iwz','38':'ke0no4tg','39':'kdr4e4wl','40':'kdjilbd3','41':'7HMG67','42':'7HV84W',\n",
    "                       '43':'7JWD4K','44':'7MBVPK','45':'7KMNHS','46':'7KL867','47':'7L87TF','48':'7MPFHS','49':'7MKFFR',\n",
    "                      '50':'7MHM4W','51':'7MJ773','52':'7NFJKW','53':'7N93QH','54':'7RMTVM','55':'7RW6GF','56':'7RWBXQ',\n",
    "                      '57':'7SDWSM','58':'7SMD93','59':'7V9Q5S','60':'7W3P7C','70':'7WGXYW','71':'2G7926','72':'k40k27he',\n",
    "                      '73':'k47r2n3q','74':'k4m6ts8z','75':'k4nsgd6d','76':'knfrg28p','77':'knfawjjo','78':'knfak07e','79':'kjtmvtd8','80':'kippk2c8','81':'kil0ffs0','82':'kia2b6g6',\n",
    "                       '83':'ki5k4qzl','84':'ki3xbmyf','85':'khzsf37x','86':'khx6uk8f','87':'khpxb8hx','88':'khpx3qrg','89':'khjvrn4u',\n",
    "                       '90':'kgq2lzjt','91':'kh604tvu','92':'kg8xc28s','93':'kg7pewto','94':'kfhtgs25','95':'kfdrhdqz','96':'kf54thjh','97':'kf3msaay',\n",
    "                       '98':'kev6cplc','99':'kev689ab','100':'kerwnszh','101':'ke9qjtwg'}\n",
    "    elif patient_list == 'AE_attack_patient':\n",
    "        patient_list = {'PR_004':'7HMG67','PR_005':'7HV84W','PR_010':'7JWD4K','PR_012':'7MBVPK','PR_014':'7KL867','PR_017':'7L87TF',\n",
    "                        'PR_023':'7MPFHS','PR_025':'7MKC44','PR_026':'7MHM4W','PR_027':'7MJ773','PR_028':'7NFJKW','PR_029':'7N93QH',\n",
    "                        'PR_034':'7RMTVM', 'PR_036':'7RW6GF','PR_037':'7RWBXQ','PR_038':'7SDWSM','PR_039':'7SMD93','PR_044':'7V9Q5S',\n",
    "                        'PR_046':'7W3P7C','PR_048':'2G7926','PR_051':'k3fnnnjn','PR_056':'k40k27he','PR_061':'k4m6ts8z','PR_070':'k773avug',\n",
    "                        'PR_074':'k7ohrrzh','PR_076':'k8nsxrf8','PR_079':'k8xykf3o','PR_080':'k997cdna','PR_082':'k99b5sby','PR_092':'kals4gfo',\n",
    "                        'PR_095':'kam6ludx','PR_097':'kavrxlv0','PR_099':'kb0gvtyo','PR_102':'kceiba9o','PR_103':'kceij9pg','PR_104':'kcyki8mp','PR_105':'kcye41i8',\n",
    "                        'PR_107':'ke0no4tg','PR_108':'kdr4e4wl', 'PR_109':'kdjilbd3', 'PR_116':'kev6cplc','PR_119':'kfdrhdqz', 'PR_121':'kg7pewto',\n",
    "                        'PR_126':'khpx3qrg','PR_127':'khpxb8hx', 'PR_128':'khx6uk8f', 'PR_129':'khzsf37x','PR_135':'kjtmvtd8','PR_137':'knfawjjo',\n",
    "                        'PR_N_002':'karou8wm', 'PR_N_004':'kb0dp1ue', 'PR_N_005':'kb1potm8', 'PR_N_008':'kbbm2myc', 'PR_N_012':'kc5xiieq','PR_N_014':'kcpnwq06',\n",
    "                        'PR_N_015':'kczuibsw', 'PR_N_016':'kczxv1yv', 'PR_N_017':'kdej4s8c',  'PR_N_018':'kdeh9tt6','PR_N_019':'kdtw95p2','PR_N_020':'kdtwh1d2',\n",
    "                        'PR_N_02':'ke2r3z50','PR_N_022':'kgd66hpt', 'PR_N_023':'khzm9k0j', 'PR_N_026':'klvk4w7v','PR_N_027':'km4vxg3p', 'PR_N_029':'kn0dlkrs'}\n",
    "    return patient_list\n",
    "\n",
    "def padding_ques(data_df, patient_list):\n",
    "    df_columns = ['Mmrc','1_move', '2_selfcare', '3_activity', '4_uncomfor', '5_depress','今天健康狀況', \n",
    "                   'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7', 'CAT_8', 'CAT_total','抗生素', '口服類固醇', '吸入短效支氣管擴張劑','value']\n",
    "    life_columns = ['serial', 'date', 'avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr',  'temperature', 'humidity', 'pm25',\n",
    "        'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex', 'NO2SubIndex', 'O38SubIndex', 'PM25SubIndex', 'wake',\n",
    "       'rem', 'deep_sleep', 'light_sleep', 'id']\n",
    "    patient_list = patient_choosing(patient_list)\n",
    "    data_list= []\n",
    "    for index,content in enumerate(patient_list):\n",
    "        range_ser = data_df['serial']==patient_list[content]\n",
    "        selected_df= data_df[data_df['serial']==patient_list[content]][df_columns]\n",
    "        data_fill = selected_df.fillna(method='ffill')\n",
    "        data_fmean = data_fill.fillna(data_fill.mean(axis=0))\n",
    "        other_df = data_df[data_df['serial']==patient_list[content]][life_columns]\n",
    "        sup = pd.concat([other_df, data_fmean], axis=1)\n",
    "        data_list.append(sup)\n",
    "    comp = pd.DataFrame()\n",
    "    for item in (data_list):\n",
    "        comp = pd.concat([comp,item],axis=0)\n",
    "    data_df=comp\n",
    "    data_df = data_df.drop_duplicates()\n",
    "#     res = data_df.to_excel('./process_file/padding_ques.xlsx', index=False)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def padding(data_df, patient_list, method='MeanFilled', filename=' '):\n",
    "    '''\n",
    "    function : padding by different methods\n",
    "    method : default as MeanFilled, 'MeanFilled', 'KNNImputer', or 'DropMiss'\n",
    "    patient_list : default as all patient, 'patient', 'NN_patient', or 'NTU_patient'\n",
    "    data_df : DataFrame type\n",
    "    '''\n",
    "    patient_list = patient_choosing(patient_list)\n",
    "    data_list= []\n",
    "    df_columns = ['Mmrc','1_move', '2_selfcare', '3_activity', '4_uncomfor', '5_depress','今天健康狀況', \n",
    "                   'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7', 'CAT_8', 'CAT_total']\n",
    "    life_columns = ['serial', 'date', 'avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr',\n",
    "        'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex', 'NO2SubIndex', 'O38SubIndex', 'PM25SubIndex', 'wake',\n",
    "       'rem', 'deep_sleep', 'light_sleep', 'id','value']\n",
    "    if method == 'MeanFilled':\n",
    "        for index,content in enumerate(patient_list):\n",
    "            range_ser = data_df['serial']==patient_list[content]\n",
    "            data_fill = data_df[range_ser].fillna(method='bfill')\n",
    "            data_fmean = data_fill.fillna(data_fill.mean(axis=0))\n",
    "            data_list.append(data_fmean)\n",
    "        comp = pd.DataFrame()\n",
    "        for item in (data_list):\n",
    "            comp = pd.concat([comp,item],axis=0)\n",
    "        data_df=comp\n",
    "    elif method == 'KNNImputer':\n",
    "        for index,content in enumerate(patient_list):\n",
    "            selected_df= data_df[data_df['serial']==patient_list[content]][df_columns]\n",
    "            other_df = data_df[data_df['serial']==patient_list[content]][life_columns]\n",
    "            data_fill = selected_df.fillna(method='bfill')\n",
    "            data_f = pd.concat([other_df,data_fill],axis=1)\n",
    "            data_list.append(data_f)\n",
    "        comp = pd.DataFrame()\n",
    "        for item in (data_list):\n",
    "            comp = pd.concat([comp,item],axis=0)\n",
    "        data_df = comp\n",
    "        info = data_df[['serial','date','id','value']]\n",
    "        data_df2=data_df.drop(['serial','date','id','value'],axis=1) \n",
    "        imputer = KNNImputer()\n",
    "        knn = imputer.fit_transform(data_df2)\n",
    "        data_knn = pd.DataFrame(knn, columns = data_df2.columns)\n",
    "        data_df = pd.concat([data_knn,info],axis=1)\n",
    "    elif method == 'DropMiss':\n",
    "        for index,content in enumerate(patient_list):\n",
    "            range_ser = data_df['serial']==patient_list[content]\n",
    "            selected_df= data_df[data_df['serial']==patient_list[content]][df_columns]\n",
    "            data_fill = selected_df.fillna(method='bfill')\n",
    "            data_life = data_df[range_ser][life_columns]\n",
    "            data_patient = pd.concat([data_life,data_fill],axis=1)\n",
    "            data_patient = data_patient.dropna()\n",
    "            data_fmean = data_fill.fillna(data_fill.mean(axis=0))\n",
    "            data_list.append(data_patient)\n",
    "        comp = pd.DataFrame()\n",
    "        for item in (data_list):\n",
    "            comp = pd.concat([comp,item],axis=0)\n",
    "        data_df=comp\n",
    "    check_missing_perc(data_df)\n",
    "    res = data_df.to_excel('./process_file/padding_'+str(filename)+'.xlsx',index=False)\n",
    "    return data_df\n",
    "\n",
    "def splitTrainTest(data_df, floder_name,test_size=0.3, file_name=' '):\n",
    "    path = './Data-'+str(floder_name)\n",
    "    mkdir(path)\n",
    "    X_train, X_test= train_test_split(data_df, shuffle = True, test_size = 0.3, random_state=987)\n",
    "    pw1 = X_train.to_excel(path+'/X_train_'+str(file_name)+'.xlsx', index=False)\n",
    "    pw2 = X_test.to_excel(path+'/X_test_'+str(file_name)+'.xlsx', index=False)\n",
    "    return X_train, X_test\n",
    "\n",
    "def splitByDate(data_df, floder_name, train_size=0.6, valid_size=0.2, file_name=' '):\n",
    "    path = './Data-'+str(floder_name)\n",
    "    mkdir(path)\n",
    "    data_df = data_df.sort_values(by='date')\n",
    "    train_date_idx = int(len(data_df['date'].unique())*(train_size))\n",
    "    valid_date_idx = int(len(data_df['date'].unique())*(train_size+valid_size))\n",
    "    train_date = data_df['date'].unique()[train_date_idx]\n",
    "    valid_date = data_df['date'].unique()[valid_date_idx]\n",
    "    print(train_date, valid_date)\n",
    "    train_df = data_df[data_df['date'] < train_date]\n",
    "    valid_df = data_df[(train_date < data_df['date']) & (data_df['date'] < valid_date)]\n",
    "    test_df = data_df[data_df['date'] > valid_date]\n",
    "    pw1 = train_df.to_excel(path+'/train_df_'+str(file_name)+'.xlsx', index=False)\n",
    "    pw2 = test_df.to_excel(path+'/test_df_'+str(file_name)+'.xlsx', index=False)\n",
    "    pw2 = valid_df.to_excel(path+'/valid_df_'+str(file_name)+'.xlsx', index=False)\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def split(data_df, floder_name,valid_size=0.3, test_size=0.3, file_name=' '):\n",
    "    path = './Data-'+str(floder_name)\n",
    "    mkdir(path)\n",
    "    X_train, X_test = train_test_split(data_df, shuffle = True, test_size = test_size, random_state=987)\n",
    "    X_train, X_valid= train_test_split(X_train, shuffle = True, test_size = valid_size, random_state=987)\n",
    "    pw1 = X_train.to_excel(path+'/X_train_'+str(file_name)+'.xlsx', index=False)\n",
    "    pw2 = X_test.to_excel(path+'/X_test_'+str(file_name)+'.xlsx', index=False)\n",
    "    pw3 = X_valid.to_excel(path+'/X_valid_'+str(file_name)+'.xlsx', index=False)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def split_into_XY(data_df):\n",
    "    Y = data_df['value']\n",
    "    X = data_df.drop(columns=['serial', 'date','id', 'value'])\n",
    "    return X,Y\n",
    "def smote(X,Y):\n",
    "    sm = SMOTE(sampling_strategy=0.6, random_state=999)\n",
    "    X_smote, Y_smote = sm.fit_resample(X,Y)\n",
    "    return X_smote, Y_smote\n",
    "    \n",
    "def split_smote(data_df, floder_name,test_size=0.3, valid_size=0.3,sampling='SMOTE', file_name=' '):\n",
    "    '''\n",
    "    function : split data into different size in random\n",
    "    data_df : DataFrame type\n",
    "    sampling : str type, 'SMOTE' or None\n",
    "    file_name : str type\n",
    "    '''\n",
    "    path = './Data-'+str(floder_name)\n",
    "    mkdir(path)\n",
    "    #先丟掉AirBox\n",
    "    data_df = data_df.dropna()\n",
    "    if 'temperature' in data_df.columns:\n",
    "        Y = data_df['value']\n",
    "        columns = ['avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr', \n",
    "                   'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex', 'NO2SubIndex',\n",
    "               'O38SubIndex', 'PM25SubIndex', 'wake', 'rem', 'deep_sleep',\n",
    "               'light_sleep', 'Mmrc', '1_move', '2_selfcare',\n",
    "               '3_activity', '4_uncomfor', '5_depress','今天健康狀況', 'CAT_1', 'CAT_2', 'CAT_3',\n",
    "               'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7', 'CAT_8', 'CAT_total',]\n",
    "        X = data_df.reindex(columns = columns)\n",
    "    #1.TF ratio fixed依照比列切出40/30/30\n",
    "    # oversampling\n",
    "    if sampling == 'SMOTE':\n",
    "        sm = SMOTE(sampling_strategy=0.6, random_state=999)\n",
    "        X_smote, Y_smote = sm.fit_resample(X,Y)\n",
    "        X_ratrain_df, X_ravalid_df, Y_ratrain_df, Y_ravalid_df= train_test_split(X_smote, Y_smote,shuffle = True, test_size = valid_size, random_state=987)\n",
    "        X_ratrain_df, X_ratest_df, Y_ratrain_df, Y_ratest_df= train_test_split(X_ratrain_df, Y_ratrain_df, shuffle = True, test_size = test_size, random_state=987)\n",
    "        xl1 = X_ratrain_df.to_excel(path+'/X_train_1v1_'+str(file_name)+'.xlsx', index=False)\n",
    "        yl1 = Y_ratrain_df.to_excel(path+'/Y_train_1v1_'+str(file_name)+'.xlsx', index=False)\n",
    "        xl2 = X_ratest_df.to_excel(path+'/X_test_1v1_'+str(file_name)+'.xlsx', index=False)\n",
    "        yl2 = Y_ratest_df.to_excel(path+'/Y_test_1v1_'+str(file_name)+'.xlsx', index=False)\n",
    "        xl3 = X_ravalid_df.to_excel(path+'/X_valid_1v1_'+str(file_name)+'.xlsx', index=False)\n",
    "        yl3 = Y_ravalid_df.to_excel(path+'/Y_valid_1v1_'+str(file_name)+'.xlsx', index=False)\n",
    "#     print(X_ratrain_df.shape, X_ravalid_df.shape,X_ratest_df.shape, Y_ratrain_df.shape, Y_ravalid_df.shape, Y_ratest_df.shape)\n",
    "#     print(Y_ratrain_df[Y_ratrain_df==True].shape,Y_ravalid_df[Y_ravalid_df==True].shape,Y_ratest_df[Y_ratest_df==True].shape)\n",
    "    elif sampling == None:\n",
    "    #2.隨機切出40/30/30\n",
    "        X_train, X_valid, Y_train, Y_valid = train_test_split(X,Y, shuffle = True, test_size = 0.3, random_state=987)\n",
    "        X_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, shuffle = True, test_size = 0.3, random_state=987)\n",
    "        pw1 = X_train.to_excel(path+'/X_train_'+str(file_name)+'.xlsx', index=False)\n",
    "        py1 = Y_train.to_excel(path+'/Y_train_'+str(file_name)+'.xlsx', index=False)\n",
    "        pw2 = X_test.to_excel(path+'/X_test_'+str(file_name)+'.xlsx', index=False)\n",
    "        py2 = Y_test.to_excel(path+'/Y_test_'+str(file_name)+'.xlsx', index=False)\n",
    "        pw3 = X_valid.to_excel(path+'/X_valid_'+str(file_name)+'.xlsx', index=False)\n",
    "        py3 = Y_valid.to_excel(path+'/Y_valid_'+str(file_name)+'.xlsx', index=False)\n",
    "#     print(X_train.shape, X_valid.shape,X_test.shape, Y_train.shape, Y_valid.shape, Y_test.shape)\n",
    "#     print(Y_train[Y_train==True].shape,Y_valid[Y_valid==True].shape,Y_test[Y_test==True].shape)\n",
    "\n",
    "def clip_outlier(data_df, cols):\n",
    "    data_df[cols]= data_df[cols].clip(lower= data_df[cols].quantile(0.15), upper= data_df[cols].quantile(0.85), axis=1)\n",
    "    return data_df\n",
    "\n",
    "def clip_outlier_total(data_df):\n",
    "    mp = data_df.reindex(columns=['avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr',\n",
    "        'AQI', 'SO2SubIndex', 'COSubIndex',\n",
    "       'PM10SubIndex', 'NO2SubIndex', 'O38SubIndex', 'PM25SubIndex', 'wake',\n",
    "       'rem', 'deep_sleep', 'light_sleep', 'Mmrc', '1_move',\n",
    "       '2_selfcare', '3_activity', '4_uncomfor', '5_depress', '今天健康狀況',\n",
    "       'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7', 'CAT_8',\n",
    "       'CAT_total'])\n",
    "    for cols in mp.columns:\n",
    "        data_df[cols]= data_df[cols].clip(lower= data_df[cols].quantile(0.15), upper= data_df[cols].quantile(0.85))\n",
    "    return data_df\n",
    "\n",
    "def normalization(X):\n",
    "    X_norm = normalize(X)\n",
    "    X_norm = pd.DataFrame(X_norm)\n",
    "    return X_norm\n",
    "\n",
    "def label_CAT(ques_df, life_df):\n",
    "    refans = life_df['value']\n",
    "    cat_ram = ques_df.reindex(columns=['id', 'date','CAT_total'])\n",
    "    cat_ram = cat_ram.dropna(subset=['CAT_total'])\n",
    "    value = []\n",
    "    for i in range(0, len(cat_ram['CAT_total'])):\n",
    "        perInfo = cat_ram['id'].iloc[i]\n",
    "        if i == 0:\n",
    "            continue\n",
    "        elif cat_ram['CAT_total'].iloc[i] + 2 >= cat_ram['CAT_total'].iloc[i-1]:\n",
    "            now = cat_ram['date'].iloc[i]\n",
    "            unc = listing_day(now)\n",
    "            for j in unc:\n",
    "                value.append([perInfo, j, True])\n",
    "    value_df = pd.DataFrame(value, columns= ['id', 'date', 'replace_value'])\n",
    "    comp = pd.merge(life_df, value_df, on=['id','date'], how = 'left')\n",
    "    origValue = comp['value'].to_list()\n",
    "    replValue = comp['replace_value'].to_list()\n",
    "    newValue = []\n",
    "    for i in range(0, len(origValue)-1):\n",
    "        if type(replValue[i]) == float:\n",
    "            newValue.append(origValue[i])\n",
    "        else:\n",
    "            newValue.append(replValue[i])\n",
    "    cimp = pd.DataFrame(newValue, columns=['value'])\n",
    "    nwl_df =life_df.reindex(columns=['serial', 'date', 'avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr',\n",
    "       'temperature', 'humidity', 'pm25', 'wake', 'rem', 'deep_sleep',\n",
    "       'light_sleep', 'id', 'Mmrc', '1_move', '2_selfcare', '3_activity',\n",
    "       '4_uncomfor', '5_depress','今天健康狀況', 'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5',\n",
    "       'CAT_6', 'CAT_7', 'CAT_8', 'CAT_total', 'AE 日期 (二週內)', 'SiteId',\n",
    "       'SiteName', 'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex',\n",
    "       'NO2SubIndex', 'O38SubIndex', 'PM25SubIndex'])\n",
    "    new_df = pd.concat([nwl_df,cimp], axis=1)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def differential(data_df):\n",
    "    origin_columns = data_df.drop(['serial', 'date', 'id', 'Mmrc', '1_move', '2_selfcare', '3_activity',\n",
    "                                   '4_uncomfor', '5_depress', 'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5',\n",
    "                                   'CAT_6', 'CAT_7', 'CAT_8', 'CAT_total', 'value'], axis=1)\n",
    "    tmp_columns = ['diff_' + i for i in origin_columns.columns]\n",
    "    tmp = pd.DataFrame([], columns=tmp_columns)\n",
    "    for id in data_df['serial'].unique():\n",
    "        tmp_data = data_df[data_df['serial'] == id]\n",
    "        tmp_data[tmp_columns] = tmp_data[origin_columns.columns].diff().fillna(value=data_df.mean())\n",
    "        tmp_data_modify = tmp_data[tmp_columns].drop_duplicates().T.drop_duplicates().T\n",
    "        tmp = tmp.append(tmp_data_modify)\n",
    "    data_df = pd.concat([data_df, tmp[tmp_columns]], axis=1)\n",
    "    tmp_columns = ['diff2_' + i for i in origin_columns.columns]\n",
    "    tmp = pd.DataFrame([], columns=tmp_columns)\n",
    "    for id in data_df['serial'].unique():\n",
    "        tmp_data = data_df[data_df['serial'] == id]\n",
    "        tmp_data[tmp_columns] = tmp_data[origin_columns.columns].diff(periods=2).fillna(value=data_df.mean())\n",
    "        tmp_data_modify = tmp_data[tmp_columns].drop_duplicates().T.drop_duplicates().T\n",
    "        tmp = tmp.append(tmp_data_modify)\n",
    "    data_df = pd.concat([data_df, tmp[tmp_columns]], axis=1)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def paddingAttenAE(data_df, patient_list, filename=' '):\n",
    "    patient_list = patient_choosing(patient_list)\n",
    "    data_list= []\n",
    "    \n",
    "    df_columns = ['Mmrc','1_move', '2_selfcare', '3_activity', '4_uncomfor', '5_depress','今天健康狀況', \n",
    "                   'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7', 'CAT_8', 'CAT_total']\n",
    "    life_columns = ['serial', 'date', 'avg_step',\n",
    "                    'calories', 'q1_hr', 'q2_hr', 'q3_hr', 'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex',\n",
    "                    'NO2SubIndex', 'O38SubIndex', 'PM25SubIndex', 'wake','rem', 'deep_sleep', 'light_sleep', 'id','value']\n",
    "    for index,content in tqdm(enumerate(patient_list)):\n",
    "        range_ser = data_df[data_df['serial']==patient_list[content]]\n",
    "        for row in range_ser[range_ser['value']==True].itertuples():\n",
    "            upper_row_index = data_df.iloc[row[0]-1][data_df.iloc[row[0]-1].isnull() == True].index\n",
    "            downner_index = data_df.iloc[row[0]][data_df.iloc[row[0]].isnull() == True].index\n",
    "\n",
    "            if (upper_row_index.all() == downner_index.all()):\n",
    "                if  (upper_row_index.all() != []) & (downner_index.all() != []):\n",
    "                    ae_row = pd.DataFrame(data_df.iloc[row[0]].fillna(range_ser[range_ser['value']==True].mean(axis=0)))\n",
    "            else:\n",
    "                ae_row = pd.DataFrame(data_df.iloc[row[0]].fillna(data_df.iloc[row[0]-1]))\n",
    "            at_tow = ae_row.T\n",
    "            data_list.append(at_tow)\n",
    "\n",
    "        false_ser = range_ser[range_ser['value']==False]\n",
    "        data_fill = false_ser.fillna(method='ffill')\n",
    "        data_fmean = data_fill.fillna(data_fill.mean(axis=0))\n",
    "        data_list.append(data_fmean)\n",
    "    comp = pd.DataFrame()\n",
    "    for item in (data_list):\n",
    "        comp = pd.concat([comp,item],axis=0)\n",
    "    data_df=comp\n",
    "    res = data_df.to_excel('./process_file/paddingAttenAE_'+str(filename)+'.xlsx',index=False)\n",
    "    return data_df\n",
    "def padding_medicine(ques_df, life_df):\n",
    "    df_columns = ['Age','anti-biotics',\n",
    "       'steroid', 'bronchodilators']\n",
    "    life_columns = ['serial', 'date', 'avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr',  'temperature', 'humidity', 'pm25',\n",
    "        'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex', 'NO2SubIndex', 'O38SubIndex', 'PM25SubIndex', 'wake',\n",
    "       'rem', 'deep_sleep', 'light_sleep', 'id']\n",
    "    \n",
    "    columns = ['id','AE 日期 (二週內)']\n",
    "    age_columns = ['id','Age']\n",
    "    comp = []\n",
    "    for u in df_columns:\n",
    "        if u == \"Age\":\n",
    "            life_df = pd.merge(life_df, ques_df[age_columns], on=['id'], how='left')\n",
    "        else:\n",
    "            bio_date = ques_df[ques_df[u].notnull()][columns]\n",
    "            ae = []\n",
    "            for id,content in bio_date.values:\n",
    "                time_sec = listing_day(content)\n",
    "                bio_data = ques_df[ques_df['AE 日期 (二週內)'] == content]\n",
    "                uni_data = bio_data[bio_data['id'] == id]\n",
    "                ae_whom = str(uni_data['id'].values).strip('[]').strip(\"''\")\n",
    "                for i in time_sec:  \n",
    "                    ae_when = str(i).strip('00:00:00').strip(' ')\n",
    "                    bio_value = True\n",
    "                    ae.append([ae_whom,ae_when,bio_value])\n",
    "            ae = pd.DataFrame(ae, columns = ['id', 'date', u])\n",
    "            ae['date'] = pd.to_datetime(ae['date'])\n",
    "            ae = ae.drop_duplicates()\n",
    "            comp = pd.merge(life_df, ae, on=['id','date'], how = 'left')\n",
    "            # input false\n",
    "            pend_num = comp[u].shape[0]\n",
    "            value_queue = []\n",
    "            for i in comp[u]:\n",
    "                if i == True:\n",
    "                    value_queue.append(True)\n",
    "                else:\n",
    "                    value_queue.append(False)\n",
    "            comp[u] = pd.DataFrame(value_queue)\n",
    "            life_df = pd.concat([life_df,comp[u]], axis=1)\n",
    "#     res = comp.to_excel('./process_file/labeling.xlsx', index=False)\n",
    "    return life_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire concatenation\n",
    "- adjust the header for questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 總院問卷\n",
    "ques_file = 'C:/Users/smiley/Desktop/COPD/總院redcap資料_2021_04_19.xlsx' \n",
    "ques_df = read(ques_file)\n",
    "comp1 = ques_df['id'].fillna(method='ffill')\n",
    "ques_df['id'] = comp1\n",
    "comp1 = ques_df['主要診斷'].fillna(method='ffill')\n",
    "ques_df['主要診斷'] = comp1\n",
    "comp1 = ques_df['Age'].fillna(method='ffill')\n",
    "ques_df['Age'] = comp1\n",
    "comp2 = ques_df.reindex(columns = [ 'id','主要診斷','Age','Mmrc', '1_move', '2_selfcare',\n",
    "       '3_activity', '4_uncomfor', '5_depress', '今天健康狀況', '　EQ-5D total score',\n",
    "       'date', 'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7',\n",
    "       'CAT_8', 'CAT_total', 'AE 日期 (二週內)','anti-biotics',\n",
    "       'steroid', 'bronchodilators'])\n",
    "comp3 = comp2.dropna(subset=['date'])\n",
    "ques_df = comp3\n",
    "ques_df = ques_df[ques_df['主要診斷']==\"COPD\"]\n",
    "ranL = []\n",
    "for i in idD['id']:\n",
    "    a = ques_df[ques_df['id']==i]\n",
    "    ranL.append(a)\n",
    "comp = pd.DataFrame()\n",
    "for item in ranL:\n",
    "    comp = pd.concat([comp, item], axis=0)\n",
    "ques_df.to_excel('C:/Users/smiley/Desktop/COPD/20210614總院問卷.xlsx')\n",
    "ques_df=comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 北護問卷\n",
    "ques_file = 'C:/Users/smiley/Desktop/COPD/台大北護工作列表_20210409.xlsx' \n",
    "qubh_df = read(ques_file)\n",
    "comp1 = qubh_df['id'].fillna(method='ffill')\n",
    "qubh_df['id'] = comp1\n",
    "comp1 = qubh_df['主要診斷'].fillna(method='ffill')\n",
    "qubh_df['主要診斷'] = comp1\n",
    "comp1 = qubh_df['Age'].fillna(method='ffill')\n",
    "qubh_df['Age'] = comp1\n",
    "\n",
    "comp2 = qubh_df.reindex(columns = [ 'id','主要診斷','Age','Mmrc', '1_move', '2_selfcare',\n",
    "       '3_activity', '4_uncomfor', '5_depress', '今天健康狀況', '　EQ-5D total score',\n",
    "       'date', 'CAT_1', 'CAT_2', 'CAT_3', 'CAT_4', 'CAT_5', 'CAT_6', 'CAT_7',\n",
    "       'CAT_8', 'CAT_total', 'AE 日期 (二週內)','anti-biotics',\n",
    "       'steroid', 'bronchodilators'])\n",
    "comp3 = comp2.dropna(subset=['date'])\n",
    "qubh_df = comp3\n",
    "qubh_df = qubh_df[qubh_df['主要診斷']==\"COPD\"]\n",
    "ranL = []\n",
    "for i in idD['id']:\n",
    "    a = qubh_df[qubh_df['id']==i]\n",
    "    ranL.append(a)\n",
    "comp = pd.DataFrame()\n",
    "for item in ranL:\n",
    "    comp = pd.concat([comp, item], axis=0)\n",
    "qubh_df.to_excel('C:/Users/smiley/Desktop/COPD/20210614北護問卷.xlsx')\n",
    "qubh_df=comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_df = pd.concat([ques_df,qubh_df], axis=0)\n",
    "ques_df.to_excel('C:/Users/smiley/Desktop/COPD/20210615ques.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file = 'C:/Users/smiley/Desktop/COPD/Questionaire/0409_mapping.xlsx'\n",
    "seg_file = 'C:/Users/smiley/Desktop/COPD/OrigData-address/withSiteName_0413_obser.xlsx' #含北護的\n",
    "open_file = 'C:/Users/smiley/Desktop/COPD/YuOpenData_0420.csv' #含北護的\n",
    "hour_file = 'C:/Users/smiley/Desktop/COPD/process_file/hour_to_avg_0519.xlsx' \n",
    "ques_file = 'C:/Users/smiley/Desktop/COPD/20210615ques.xlsx'\n",
    "# hourbh_file = 'C:/Users/smiley/Desktop/COPD/process_file/hour_to_avg_bh.xlsx' \n",
    "\n",
    "map_df = read(map_file)\n",
    "seg_df = read(seg_file)\n",
    "open_df = read(open_file)\n",
    "hour_df = read(hour_file)\n",
    "ques_df = read(ques_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID combination finish\n"
     ]
    }
   ],
   "source": [
    "id_df = append_id(hour_df)\n",
    "id_df.to_excel('./process_file/0616_id.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = read('./process_file/0610_id.xlsx')\n",
    "mp_df = map_ques(id_df, ques_df)\n",
    "mp_df.to_excel('./process_file/0616_map.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = extract_hos_time(seg_df, mp_df)\n",
    "time_df.to_excel('./process_file/0616time_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "openhs_df = merge_open_data(open_df, time_df)\n",
    "openhs_df = openhs_df.drop_duplicates()\n",
    "openhs_df.to_excel('./process_file/0616openhs_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = labeling(ques_df, openhs_df)\n",
    "label_df.to_excel('./process_file/0616label_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the shape of ground truth on first labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 48)"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df[label_df['value']==True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = label_CAT(ques_df, label_df)\n",
    "cat_df.to_excel('./process_file/0616cat_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the shape of ground truth on second labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728, 42)"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df[cat_df['value']==True].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values\n",
    "- drop_all_data\n",
    "- padding_all_data\n",
    "    - filename: String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all_data(data_df):\n",
    "    data_df= data_df.dropna()\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = drop_all_data(cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_all_data(data_df, filename):\n",
    "    pad_df = paddingAttenAE(data_df, 'patient', filename=filename)\n",
    "    return pad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = padding_all_data(cat_df, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nornmalization(data_df):\n",
    "    from sklearn import preprocessing\n",
    "    scaler = data_df.reindex(columns=['avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr',\n",
    "           'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex', 'NO2SubIndex',\n",
    "           'PM25SubIndex', 'wake', 'rem', 'deep_sleep', 'light_sleep'])\n",
    "    infodf = data_df.reindex(columns=['serial', 'date', 'id','value','anti-biotics', 'steroid', 'bronchodilators'])\n",
    "\n",
    "    Min_Max_Scaler = preprocessing.MinMaxScaler(feature_range=(0,1)) # 設定縮放的區間上下限\n",
    "    MinMax_Data = Min_Max_Scaler.fit_transform(scaler)\n",
    "    \n",
    "    nor_df = pd.DataFrame(MinMax_Data, columns=['avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr',\n",
    "       'AQI', 'SO2SubIndex', 'COSubIndex', 'PM10SubIndex', 'NO2SubIndex',\n",
    "       'PM25SubIndex', 'wake', 'rem', 'deep_sleep', 'light_sleep'])\n",
    "    data_df = pd.concat([nor_df,infodf], axis =1)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = nornmalization(comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df=comp_df.reindex(columns=['serial', 'date', 'Age', \n",
    "       'avg_step', 'calories', 'q1_hr', 'q2_hr', 'q3_hr', 'AQI', 'SO2SubIndex',\n",
    "       'COSubIndex', 'PM10SubIndex', 'NO2SubIndex', 'PM25SubIndex', 'wake',\n",
    "       'rem', 'deep_sleep', 'light_sleep', 'id', 'value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "1. split by ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = split(comp_df, 'Folder_name',test_size=0.1, valid_size=0.1, file_name='ntu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. split by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = splitByDate(comp_df, 'Folder_name', train_size=0.6, test_size=0.2, file_name='ntu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = split_into_XY(train_df)\n",
    "Ytrain=Ytrain.astype('int')\n",
    "XsmoteTrain, YsmoteTrain = smote(Xtrain,Ytrain)\n",
    "\n",
    "Xvalid, Yvalid = split_into_XY(valid_df)\n",
    "Yvalid=Yvalid.astype('int')\n",
    "XsmoteValid, YsmoteValid = smote(Xvalid,Yvalid)\n",
    "\n",
    "Xtest, Ytest = split_into_XY(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xtrain & Ytrain: original training set\n",
    "- Xvalid & Yvalid: original validating set\n",
    "- XsmoteTrain & YsmoteTrain: oversampling training set\n",
    "- XsmoteValid & YsmoteValid: oversampling validating set\n",
    "- Xtest & Ytest: original testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.to_excel('./Folder_name/Xtrain.xlsx', index=False)\n",
    "Ytrain.to_excel('./Folder_name/Ytrain.xlsx', index=False)\n",
    "Xvalid.to_excel('./Folder_name/Xvalid.xlsx', index=False)\n",
    "Yvalid.to_excel('./Folder_name/Yvalid.xlsx', index=False)\n",
    "XsmoteTrain.to_excel('./Folder_name/XsmoteTrain.xlsx', index=False)\n",
    "YsmoteTrain.to_excel('./Folder_name/YsmoteTrain.xlsx', index=False)\n",
    "XsmoteValid.to_excel('./Folder_name/XsmoteTest.xlsx', index=False)\n",
    "YsmoteValid.to_excel('./Folder_name/YsmoteTest.xlsx', index=False)\n",
    "Xtest.to_excel('./Folder_name/Xtest.xlsx', index=False)\n",
    "Ytest.to_excel('./Folder_name/Ytest.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
